#pragma once

#include "mark_entity_rx_impl.h"
#include "mark_entity_tx_impl.h"
#include "mark_session_logger.h"
#include "srsran/mark/mark.h"
#include "srsran/support/timers.h"
#include <unordered_map>
#include "srsran/ctsa/ctsa.h"
#include <thread>
#include <atomic>
#include <mutex>
#include <condition_variable>

namespace srsran {

namespace srs_cu_up {

class mark_entity_impl : public mark_entity, 
                         public mark_tx_sdu_handler,
                        //  public mark_rx_m1_interface,
                        //  public mark_tx_m1_interface
                         public mark_tx_lower_interface,
                         public mark_rx_lower_interface
{
public:
  mark_entity_impl(uint32_t ue_index_, 
                   pdu_session_id_t psi_, 
                   mark_rx_sdu_notifier& rx_sdu_notifier_,
                   uint32_t nof_drbs) :
    logger("MARK", {ue_index_, psi_}), 
    ue_index(ue_index_), 
    psi(psi_), 
    rx_sdu_notifier(rx_sdu_notifier_),
    periodic_timer_running(false)
  {
    dequeue_rate_cal_wind = 50;
    dequeue_rate_pred_wind = 50;
    l4s_tq_thr = 10000; // 10000 ns = 10 ms;
    classic_tq_thr = 100000; // 50000 ns = 50 ms;
    dequeue_history = (double*)malloc(sizeof(double) * dequeue_rate_pred_wind);
    n_max = 1500*150;
    nof_ue = 1;
    
    // Start periodic timer thread
    start_periodic_timer();
  }
  
  ~mark_entity_impl() override {
    stop_periodic_timer();
  }

  mark_rx_pdu_handler& get_mark_rx_pdu_handler() final{ return *rx.get(); };
  mark_tx_sdu_handler& get_mark_tx_sdu_handler() final { return *this; };
  // mark_rx_m1_interface& get_mark_rx_m1_interface() final { return *this; };
  // mark_tx_m1_interface& get_mark_tx_m1_interface() final { return *this; };
  mark_rx_lower_interface& get_mark_rx_lower_interface() final { return *this; };
  mark_tx_lower_interface& get_mark_tx_lower_interface() final { return *this; };

  /// Handle the incoming SDU and redirect to mapped DRB.
  void handle_sdu(byte_buffer sdu, qos_flow_id_t qos_flow_id) final
  {
    logger.log_info("TX PDU. pdu_len={}, qfi={}", sdu.length(), qos_flow_id);
    auto now = std::chrono::system_clock::now();
    auto duration = now.time_since_epoch();
    auto ts = std::chrono::duration_cast<std::chrono::microseconds>(duration);

    auto sdu_it = sdu.segments().begin();
    std::stringstream tmp_string;
    tmp_string << std::hex << std::setfill('0');
    while (sdu_it != sdu.segments().end()) 
    {
      // Extract IP header and TCP/UDP header
      drb_id_t drb_id = qfi_to_drb[qos_flow_id];
      iphdr* ipv4_hdr = (iphdr*)malloc(sizeof(iphdr));
      memcpy(ipv4_hdr, (*sdu_it).data(), sizeof(iphdr));
      ip::swap_iphdr(ipv4_hdr);
      ip::five_tuple pkt_five_tuple = {};
      if (ipv4_hdr->protocol == 6) {
        tcphdr* tcp_hdr = (tcphdr*)malloc(sizeof(tcphdr));
        memcpy(tcp_hdr, (*sdu_it).data()+sizeof(iphdr), sizeof(tcphdr));
        ip::swap_tcphdr(tcp_hdr);
        pkt_five_tuple = ip::extract_five_tuple(*ipv4_hdr, *tcp_hdr);

        // Flow mapping: map 5 tuples of IP/TCP flow to DRB id.
        rx.get()->five_tuple_to_drb[pkt_five_tuple].drb_id = drb_id;
        
        // Track TCP packets with payload for in-flight monitoring
        uint16_t tcp_payload_len = ipv4_hdr->tot_len - sizeof(iphdr) - (tcp_hdr->doff * 4);
        if (tcp_payload_len > 0 && !(uint8_t)tcp_hdr->syn && !(uint8_t)tcp_hdr->rst) {
          // Only track data packets (not pure ACKs, SYN, or RST)
          ip::tcp_packet_info pkt_info(tcp_hdr->seq, tcp_payload_len, ipv4_hdr->tot_len, ts.count());
          pkt_info.fake_acked = false;  // Initialize fake_acked flag
          
          // Copy the complete IP packet data for potential retransmission or deep inspection
          pkt_info.packet_data.resize(ipv4_hdr->tot_len);
          memcpy(pkt_info.packet_data.data(), (*sdu_it).data(), ipv4_hdr->tot_len);
          
          // Check if this is a retransmission, This is generated by AI, may need further optimization, ZXZ
          auto& flow_track = rx.get()->tcp_flow_tracking[pkt_five_tuple];
          for (const auto& in_flight_pkt : flow_track.in_flight_packets) {
            if (tcp_hdr->seq == in_flight_pkt.seq_num) {
              pkt_info.is_retransmission = true;
              flow_track.total_retransmissions++;
              logger.log_debug("TCP retransmission detected: seq={}, flow={}", tcp_hdr->seq, pkt_five_tuple);
              break;
            }
          }

          // Add to in-flight queue
          flow_track.in_flight_packets.push_back(std::move(pkt_info));
          flow_track.total_packets_sent++;
          flow_track.last_tx_timestamp_us = ts.count();
          flow_track.next_expected_seq = tcp_hdr->seq + tcp_payload_len;
          // Important Log1 ZXZ
          logger.log_debug("TX TCP packet tracked: seq={}, len={}, pkt_size={}, in_flight={}, flow={}", 
                          tcp_hdr->seq, tcp_payload_len, ipv4_hdr->tot_len,
                          flow_track.get_packets_in_flight(), pkt_five_tuple);
        }
        
        // Insert the packet into the DRB queue
        drb_queue_update(*ipv4_hdr, drb_id, ts, pkt_five_tuple);
        update_drb_flow_state_tcp(*ipv4_hdr, *tcp_hdr, drb_id, ts);
      } else if (ipv4_hdr->protocol == 17) {
        udphdr* udp_hdr = (udphdr*)malloc(sizeof(udphdr));
        memcpy(udp_hdr, (*sdu_it).data()+sizeof(iphdr), sizeof(udphdr));
        ip::swap_udphdr(udp_hdr);
        pkt_five_tuple = ip::extract_five_tuple(*ipv4_hdr, *udp_hdr);

        // Flow mapping: map 5 tuples of IP/TCP flow to DRB id.
        rx.get()->five_tuple_to_drb[pkt_five_tuple].drb_id = drb_id;

        // Insert the packet into the DRB queue
        drb_queue_update(*ipv4_hdr, drb_id, ts, pkt_five_tuple);
        update_drb_flow_state_udp(*ipv4_hdr, *udp_hdr, drb_id, ts);
      } else {
        // Insert the packet into the DRB queue
        drb_queue_update(*ipv4_hdr, drb_id, ts, pkt_five_tuple);
      }
      sdu_it ++;
    }
    tx.get()->handle_sdu(std::move(sdu), qos_flow_id);
  }

  void create_rx() final
  {
    rx = std::make_unique<mark_entity_rx_impl>(ue_index, psi, rx_sdu_notifier);
  }

  void create_tx(mark_tx_pdu_notifier& tx_pdu_notifier) final
  {
    tx = std::make_unique<mark_entity_tx_impl>(ue_index, psi, tx_pdu_notifier);
  }

  void add_drb(drb_id_t drb_id, pdcp_rlc_mode rlc_mod) final
  {
    drb_rlc[drb_id] = rlc_mod;
    pdcp_sn_sizes[drb_id] = 0;
    pdcp_sn_maxs[drb_id] = 0;
    next_tx_id[drb_id] = 0;
    next_delivery_id[drb_id] = 0;
    rx->drb_flow_state[drb_id] = {};
  }

  void add_mapping(qos_flow_id_t qfi, drb_id_t drb_id) final
  {
    qfi_to_drb[qfi] = drb_id;
  }

  void set_pdcp_sn_size(drb_id_t drb_id, uint8_t sn_size) final {
    pdcp_sn_sizes[drb_id] = sn_size;
    pdcp_sn_maxs[drb_id] = 1 << sn_size;
  }

  // Handle the feedback from the NR-U interface, it's called by another executor,
  // so it won't affect the downlink or uplink traffic performance.
  void handle_feedback(mark_utils::delivery_status_feedback feedback, drb_id_t drb_id_) final 
  {
    logger.log_info("Received feedback for {}", drb_id_);
    
    bool change_mark_flag = false;
    auto now = std::chrono::system_clock::now();
    auto duration = now.time_since_epoch();
    auto timestamp = std::chrono::duration_cast<std::chrono::microseconds>(duration);

    // TODO_HW: maybe remove some too old entries to save memory.

    if (feedback.highest_pdcp_sn_retransmitted != 0) {
      // TODO_HW: add wrap around determination
      // logger.log_debug("next_tx_id {}, current feedback {}", next_tx_id[drb_id_], feedback.highest_pdcp_sn_retransmitted);
      double total_size = 0;
      double total_time = 0;
      double dequeue_rate = 0;

      // Update the dequeue rate based on the feedback
      if (next_tx_id[drb_id_] == 0) {
        dequeue_rate = 0;
      } else {
        for (size_t i = next_tx_id[drb_id_]; i < drb_pdcp_sn_ts[drb_id_].size(); i++) {
          if (drb_pdcp_sn_ts[drb_id_][i].pdcp_sn <= feedback.highest_pdcp_sn_transmitted){
            total_size += drb_pdcp_sn_ts[drb_id_][i].size;
          }
        }
        total_time = (double)(timestamp - drb_pdcp_sn_ts[drb_id_][next_tx_id[drb_id_] - 1].transmitted_time).count();
        dequeue_rate = total_size / total_time;
        if (total_time < 1000) {
          dequeue_rate = drb_pdcp_sn_ts[drb_id_][next_tx_id[drb_id_]-1].cal_dequeue_rate;
        }
      }

      // Update the dequeue rate estimation errors
      for (size_t i = next_tx_id[drb_id_]; i < drb_pdcp_sn_ts[drb_id_].size(); i++) {
        if (drb_pdcp_sn_ts[drb_id_][i].pdcp_sn <= feedback.highest_pdcp_sn_retransmitted) {
          drb_pdcp_sn_ts[drb_id_][i].transmitted_time = timestamp;
          // if (dequeue_rate != 0) {
          drb_pdcp_sn_ts[drb_id_][i].cal_dequeue_rate = dequeue_rate; //calculate_dequeue_rate(i, drb_id_);
          if (drb_pdcp_sn_ts[drb_id_][i].pred_dequeue_rate > 0) {
            drb_pdcp_sn_ts[drb_id_][i].dequeue_rate_error = drb_pdcp_sn_ts[drb_id_][i].cal_dequeue_rate - drb_pdcp_sn_ts[drb_id_][i].pred_dequeue_rate;
            logger.log_debug("current_ts:{}, drb_id:{}, i:{}, ingress:{}, dequeue_rate_pred:{}, dequeue_rate_cal:{}, error_esti:{}, error:{}",
              timestamp.count(),
              drb_id_, i, drb_pdcp_sn_ts[drb_id_][i].ingress_time.count(), 
              drb_pdcp_sn_ts[drb_id_][i].pred_dequeue_rate, drb_pdcp_sn_ts[drb_id_][i].cal_dequeue_rate,
              drb_pdcp_sn_ts[drb_id_][i].est_dequeue_rate_error, drb_pdcp_sn_ts[drb_id_][i].dequeue_rate_error);
          }
          // } else {
          //   drb_pdcp_sn_ts[drb_id_][i].cal_dequeue_rate = dequeue_rate; //calculate_dequeue_rate(i, drb_id_);
          // }
          // logger.log_debug("Current {} calculated dequeue rate {}", i, drb_pdcp_sn_ts[drb_id_][i].cal_dequeue_rate);
          drb_pdcp_sn_ts[drb_id_][i].queue_delay = (timestamp - drb_pdcp_sn_ts[drb_id_][i].ingress_time).count();
          if (drb_pdcp_sn_ts[drb_id_][i].est_queue_delay > 0) {
            drb_pdcp_sn_ts[drb_id_][i].queue_delay_error = drb_pdcp_sn_ts[drb_id_][i].queue_delay - drb_pdcp_sn_ts[drb_id_][i].est_queue_delay;
          }
        } else {
          break;
        }
      }
      
      if (next_tx_id[drb_id_] < feedback.highest_pdcp_sn_retransmitted + 1) {
        change_mark_flag = true;
        next_tx_id[drb_id_] = feedback.highest_pdcp_sn_retransmitted + 1;
      }
    }

    if (feedback.highest_pdcp_sn_delivered_retransmitted != 0) {
      // logger.log_debug("next_delivery_id {}, current feedback {}", next_delivery_id[drb_id_], feedback.highest_pdcp_sn_delivered_retransmitted);
      // TODO_HW: add wrap around determination
      for (size_t i = next_delivery_id[drb_id_]; i < drb_pdcp_sn_ts[drb_id_].size(); i++) {
        if (drb_pdcp_sn_ts[drb_id_][i].pdcp_sn <= feedback.highest_pdcp_sn_delivered_retransmitted) {
          drb_pdcp_sn_ts[drb_id_][i].delivered_time = timestamp;
        } else {
          break;
        }
      }
      next_delivery_id[drb_id_] = feedback.highest_pdcp_sn_delivered_retransmitted + 1;
    }

    if (feedback.highest_pdcp_sn_transmitted != 0) {
      // logger.log_debug("next_tx_id {}, current feedback {}", next_tx_id[drb_id_], feedback.highest_pdcp_sn_transmitted);
      // TODO_HW: add wrap around determination
      double total_size = 0;
      double total_time = 0;
      double dequeue_rate = 0;

      if (next_tx_id[drb_id_] == 0) {
        dequeue_rate = 0;
      } else {
        // next_tx_id[drb_id_]-1 > 0
        for (size_t i = next_tx_id[drb_id_]; i < drb_pdcp_sn_ts[drb_id_].size(); i++) {
          if (drb_pdcp_sn_ts[drb_id_][i].pdcp_sn <= feedback.highest_pdcp_sn_transmitted){
            total_size += drb_pdcp_sn_ts[drb_id_][i].size;
          }
        }
        total_time = (double)(timestamp - drb_pdcp_sn_ts[drb_id_][next_tx_id[drb_id_] - 1].transmitted_time).count();
        dequeue_rate = total_size / total_time;
        if (total_time < 1000) {
          dequeue_rate = drb_pdcp_sn_ts[drb_id_][next_tx_id[drb_id_]-1].cal_dequeue_rate;
        }
      }

      for (size_t i = next_tx_id[drb_id_]; i < drb_pdcp_sn_ts[drb_id_].size(); i++) {
        if (drb_pdcp_sn_ts[drb_id_][i].pdcp_sn <= feedback.highest_pdcp_sn_transmitted){
          drb_pdcp_sn_ts[drb_id_][i].transmitted_time = timestamp;
          // if (dequeue_rate != 0) {
          drb_pdcp_sn_ts[drb_id_][i].cal_dequeue_rate = dequeue_rate; //calculate_dequeue_rate(i, drb_id_);
          if (drb_pdcp_sn_ts[drb_id_][i].pred_dequeue_rate > 0) {
            drb_pdcp_sn_ts[drb_id_][i].dequeue_rate_error = drb_pdcp_sn_ts[drb_id_][i].cal_dequeue_rate - drb_pdcp_sn_ts[drb_id_][i].pred_dequeue_rate;
            logger.log_debug("current_ts:{}, drb_id:{}, i:{}, ingress:{}, dequeue_rate_pred:{}, dequeue_rate_cal:{}, error_esti:{}, error:{}",
              timestamp.count(),
              drb_id_, i, drb_pdcp_sn_ts[drb_id_][i].ingress_time.count(), 
              drb_pdcp_sn_ts[drb_id_][i].pred_dequeue_rate, drb_pdcp_sn_ts[drb_id_][i].cal_dequeue_rate,
              drb_pdcp_sn_ts[drb_id_][i].est_dequeue_rate_error, drb_pdcp_sn_ts[drb_id_][i].dequeue_rate_error);
          }
          // } else {
            // drb_pdcp_sn_ts[drb_id_][i].cal_dequeue_rate = dequeue_rate; //calculate_dequeue_rate(i, drb_id_);
          // }
          // logger.log_debug("Current {} calculated dequeue rate {}", i, drb_pdcp_sn_ts[drb_id_][i].cal_dequeue_rate);
          drb_pdcp_sn_ts[drb_id_][i].queue_delay = (timestamp - drb_pdcp_sn_ts[drb_id_][i].ingress_time).count();
          if (drb_pdcp_sn_ts[drb_id_][i].est_queue_delay > 0) {
            drb_pdcp_sn_ts[drb_id_][i].queue_delay_error = drb_pdcp_sn_ts[drb_id_][i].queue_delay - drb_pdcp_sn_ts[drb_id_][i].est_queue_delay;
          }
        } else {
          break;
        }
      }
      if (next_tx_id[drb_id_] < feedback.highest_pdcp_sn_transmitted + 1) {
        change_mark_flag = true;
        next_tx_id[drb_id_] = feedback.highest_pdcp_sn_transmitted + 1;
      }
    }

    if (feedback.highest_pdcp_sn_delivered != 0) {
      // logger.log_debug("next_delivery_id {}, current feedback {}", next_tx_id[drb_id_] - 1, feedback.highest_pdcp_sn_transmitted);
      // TODO_HW: add wrap around determination
      for (size_t i = next_delivery_id[drb_id_]; i < drb_pdcp_sn_ts[drb_id_].size(); i++) {
        if (drb_pdcp_sn_ts[drb_id_][i].pdcp_sn <= feedback.highest_pdcp_sn_delivered) {
          drb_pdcp_sn_ts[drb_id_][i].delivered_time = timestamp;
        } else {
          break;
        }
      }
      next_delivery_id[drb_id_] = feedback.highest_pdcp_sn_delivered + 1;
    }

    if (change_mark_flag) {
      make_mark_decision(drb_id_);
    }
    logger.log_info("Finished feedback for {}", drb_id_);
  }

private:
  mark_session_logger   logger;
  uint32_t              ue_index;
  pdu_session_id_t      psi;
  mark_rx_sdu_notifier& rx_sdu_notifier;

  // std::unique_ptr<m1_cu_up_gateway_bearer> m1_gw_bearer;
  // std::unique_ptr<m1_bearer>               m1;

  std::unique_ptr<mark_entity_tx_impl> tx;
  std::unique_ptr<mark_entity_rx_impl> rx;

  /// @brief: QoS to DRB mapping
  std::unordered_map<qos_flow_id_t, drb_id_t> qfi_to_drb;

  /// @brief: Ingress queue state
  std::unordered_map<drb_id_t, std::vector<mark_utils::pdcp_sn_size_ts>> drb_pdcp_sn_ts;
  /// @brief: History queue state
  // std::unordered_map<drb_id_t, std::vector<mark_utils::pdcp_sn_size_ts>> drb_pdcp_sn_ts_egress;

  /// @brief A pointer to the current queue head, next packet to be delivered through RLC,
  /// absolute idx in drb_pdcp_sn_ts_ingress and drb_pdcp_sn_ts_egress.
  std::unordered_map<drb_id_t, size_t> next_delivery_id;
  /// @brief A pointer to the current queue head, next packet to be sent through RLC,
  /// absolute idx in drb_pdcp_sn_ts_ingress and drb_pdcp_sn_ts_egress.
  std::unordered_map<drb_id_t, size_t> next_tx_id;

  std::unordered_map<drb_id_t, uint32_t> next_pdcp_sn;

  /// @brief Initially, set the pdcp sequence bit size to be 18, if we find a wrap around at 4096,
  /// we reset this to 12.
  std::unordered_map<drb_id_t, uint8_t> pdcp_sn_sizes;
  std::unordered_map<drb_id_t, uint32_t> pdcp_sn_maxs;

  std::unordered_map<drb_id_t, pdcp_rlc_mode> drb_rlc;

  /// @brief window side used to calculate the packet dequeue rate
  size_t dequeue_rate_cal_wind;
  /// @brief window side used to predict the packet dequeue rate
  size_t dequeue_rate_pred_wind;

  double* dequeue_history;

  double* dequeue_xpred;
  double* dequeue_amse;

  double l4s_tq_thr;
  double classic_tq_thr;
  uint32_t n_max;

  // Periodic timer for RLC queue monitoring and TCP ACK generation
  std::thread periodic_timer_thread;
  std::atomic<bool> periodic_timer_running;
  std::mutex data_mutex;  // Protect shared data structures
  std::condition_variable timer_cv;
  
  // Statistics for periodic monitoring
  struct periodic_stats {
    uint64_t total_checks = 0;
    uint64_t acks_generated = 0;
    int64_t last_check_timestamp_us = 0;
  } periodic_stats;

  // called upon receiving a downlink packet
  void drb_queue_update(iphdr ipv4_hdr, drb_id_t drb_id, std::chrono::microseconds now, ip::five_tuple f_tuple)
  {
    mark_utils::pdcp_sn_size_ts new_pdcp_pkt = {};
    new_pdcp_pkt.pdcp_sn = next_pdcp_sn[drb_id] % pdcp_sn_maxs[drb_id];
    next_pdcp_sn[drb_id] += 1;
    new_pdcp_pkt.size = ipv4_hdr.tot_len;
    new_pdcp_pkt.ingress_time = now;
    new_pdcp_pkt.five_tuple = f_tuple;
    drb_pdcp_sn_ts[drb_id].push_back(new_pdcp_pkt);
  }

  void update_drb_flow_state_tcp(iphdr ipv4_hdr, tcphdr hdr, drb_id_t drb_id, std::chrono::microseconds now) {
    if (ip::classify_flow(ipv4_hdr) == ip::L4S_FLOW) {
      // logger.log_debug("l4s drb_id {}, hdr.syn {}, {}", drb_id, (uint8_t)hdr.syn, ((uint8_t)hdr.syn) == 0);
      if (ipv4_hdr.protocol == 6 && ((uint8_t)hdr.syn) == 0) {
        rx.get()->drb_flow_state[drb_id].have_l4s = true;
        rx.get()->drb_flow_state[drb_id].l4s_last_see = now;
      }
    } else {
      // logger.log_debug("classic drb_id {}, hdr.syn {}, {}", drb_id, (uint8_t)hdr.syn, ((uint8_t)hdr.syn) == 0);
      if (ipv4_hdr.protocol == 6 && ((uint8_t)hdr.syn) == 0){
        rx.get()->drb_flow_state[drb_id].have_classic = true;
        rx.get()->drb_flow_state[drb_id].classic_last_see = now;
      }
        
    }

    // flow liveness check
    if ((now - rx.get()->drb_flow_state[drb_id].l4s_last_see).count() > 1000000) 
      rx.get()->drb_flow_state[drb_id].have_l4s = false;

    if ((now - rx.get()->drb_flow_state[drb_id].classic_last_see).count() > 1000000) 
      rx.get()->drb_flow_state[drb_id].have_classic = false;
  }

  void update_drb_flow_state_udp(iphdr ipv4_hdr, udphdr hdr, drb_id_t drb_id, std::chrono::microseconds now) {
    if (ip::classify_flow(ipv4_hdr) == ip::L4S_FLOW) {
      // logger.log_debug("l4s drb_id {}, udp", drb_id);
      if (ipv4_hdr.protocol == 17) {
        rx.get()->drb_flow_state[drb_id].have_l4s = true;
        rx.get()->drb_flow_state[drb_id].l4s_last_see = now;
      }
    } else {
      // logger.log_debug("classic drb_id {}, udp", drb_id);
      if (ipv4_hdr.protocol == 17) {
        rx.get()->drb_flow_state[drb_id].have_classic = true;
        rx.get()->drb_flow_state[drb_id].classic_last_see = now;
      }
    }

    // flow liveness check
    if ((now - rx.get()->drb_flow_state[drb_id].l4s_last_see).count() > 1000000) 
      rx.get()->drb_flow_state[drb_id].have_l4s = false;

    if ((now - rx.get()->drb_flow_state[drb_id].classic_last_see).count() > 1000000) 
      rx.get()->drb_flow_state[drb_id].have_classic = false;
  }

  // update the dequeue rate calculation up to i-th packet, in bytes per micro-second
  double calculate_dequeue_rate(size_t index, drb_id_t drb_id) {
    if (index == 0) {
      // for the first data, we can't calculate the dequeue rate
      return 0;
    }

    double total_sz = 0;

    if (index < dequeue_rate_cal_wind) {
      for (size_t i = 1; i <= index; i ++ ){
        total_sz += drb_pdcp_sn_ts[drb_id][i].size;
      }
      return total_sz / (double)(drb_pdcp_sn_ts[drb_id][index].transmitted_time - 
        drb_pdcp_sn_ts[drb_id][0].transmitted_time).count();
    } else {
      for (size_t i = index - dequeue_rate_cal_wind + 1; i <= index; i ++) {
        total_sz += drb_pdcp_sn_ts[drb_id][i].size;
      }
      return total_sz / (double)(drb_pdcp_sn_ts[drb_id][index].transmitted_time - 
        drb_pdcp_sn_ts[drb_id][index - dequeue_rate_cal_wind].transmitted_time).count();
    }
  }

  /// @brief Predict the sending for the queue tail packet's queuing delay.
  /// The time series window size is dequeue_rate_pred_wind, the latest observation is next_tx_id[drb]-1,
  /// and the unobserved data is from next_tx_id[drb] to drb_pdcp_sn_ts[drb].queue_tail().
  /// At least, we should use the 1 to dequeue_rate_pred_wind as the observation window.
  void predict_dequeue_rate(drb_id_t drb_id) 
  { 
    auto& vec = drb_pdcp_sn_ts[drb_id];
    if (vec.empty()) return;
    
    if (next_tx_id[drb_id] == 0) {
      // we don't expect to reach this 
      drb_pdcp_sn_ts[drb_id][next_tx_id[drb_id]].pred_dequeue_rate = 0;
      return;
    }

    if (next_tx_id[drb_id] - 1 < dequeue_rate_pred_wind) {
      // current window size is smaller than the dequeue rate predition window size
      // fallback to just an average over the past dequeue rate
      double pred_dq_rate = 0;
      double dq_std = 0;
      for (size_t i = 1; i < next_tx_id[drb_id]; i ++) {
        pred_dq_rate += drb_pdcp_sn_ts[drb_id][i].cal_dequeue_rate / (double)(next_tx_id[drb_id] - 1);
      }
      for (size_t i = 1; i < next_tx_id[drb_id]; i ++) {
        dq_std += (pred_dq_rate - drb_pdcp_sn_ts[drb_id][i].cal_dequeue_rate) * (pred_dq_rate - drb_pdcp_sn_ts[drb_id][i].cal_dequeue_rate) / (double)(next_tx_id[drb_id] - 1);
      }
      dq_std = sqrt(dq_std);

      drb_pdcp_sn_ts[drb_id].back().pred_dequeue_rate = pred_dq_rate;
      drb_pdcp_sn_ts[drb_id].back().est_dequeue_rate_error = dq_std;
      // logger.log_debug("Finished predicting the average dqueue rate {} bytes / ns.", 
        // pred_dq_rate);
    } else {
      double pred_dq_rate = 0;
      double dq_std = 0;
      for (size_t i = next_tx_id[drb_id] - dequeue_rate_pred_wind; i < next_tx_id[drb_id]; i ++) {
        pred_dq_rate += drb_pdcp_sn_ts[drb_id][i].cal_dequeue_rate / (double)dequeue_rate_pred_wind;
        //printf("cal_dequeue_rate: %.2f\n", drb_pdcp_sn_ts[drb_id][i].cal_dequeue_rate);
      }
      for (size_t i = next_tx_id[drb_id] - dequeue_rate_pred_wind; i < next_tx_id[drb_id]; i ++) {
        dq_std += (pred_dq_rate - drb_pdcp_sn_ts[drb_id][i].cal_dequeue_rate) * (pred_dq_rate - drb_pdcp_sn_ts[drb_id][i].cal_dequeue_rate) / (double)dequeue_rate_pred_wind;
      }
      dq_std = sqrt(dq_std);

      drb_pdcp_sn_ts[drb_id].back().pred_dequeue_rate = pred_dq_rate;
      drb_pdcp_sn_ts[drb_id].back().est_dequeue_rate_error = dq_std;
      // logger.log_debug("Finished predicting the average dqueue rate {} bytes / ns.", 
        // pred_dq_rate);

    //   // predict with the ARIMA first, then shift to the PID controller method
    //   logger.log_debug("Start predicting using the ARIMA method.");
    //   for (size_t i = next_tx_id[drb_id] - dequeue_rate_pred_wind; i < next_tx_id[drb_id]; i ++) {
    //     dequeue_history[i-next_tx_id[drb_id]+dequeue_rate_pred_wind] = drb_pdcp_sn_ts[drb_id][i].cal_dequeue_rate;
    //   }
    //   arima_exec(arima_obj, dequeue_history);
    //   logger.log_debug("Finished training the ARIMA method.");
    //   arima_summary(arima_obj);
    //   dequeue_xpred = (double*)calloc(drb_pdcp_sn_ts[drb_id].size() - next_tx_id[drb_id], sizeof(double));
    //   dequeue_amse = (double*)calloc(drb_pdcp_sn_ts[drb_id].size() - next_tx_id[drb_id], sizeof(double));
    //   arima_predict(arima_obj, dequeue_history, drb_pdcp_sn_ts[drb_id].size() - next_tx_id[drb_id], dequeue_xpred, dequeue_amse);
    //   logger.log_debug("Finished predicting the ARIMA method dqueue rate {} bytes / ns.", 
    //     dequeue_xpred[drb_pdcp_sn_ts[drb_id].size() - next_tx_id[drb_id]-1]);

    //   drb_pdcp_sn_ts[drb_id][drb_pdcp_sn_ts[drb_id].size()-1].pred_dequeue_rate = dequeue_xpred[drb_pdcp_sn_ts[drb_id].size() - next_tx_id[drb_id] - 1];
    //   drb_pdcp_sn_ts[drb_id][drb_pdcp_sn_ts[drb_id].size()-1].est_dequeue_rate_error = dequeue_amse[drb_pdcp_sn_ts[drb_id].size() - next_tx_id[drb_id] - 1];
    }
  }

  void predict_queuing_delay(drb_id_t drb_id) 
  {
    double standing_queue_sz = 0;
    for (size_t i = next_tx_id[drb_id]; i < drb_pdcp_sn_ts[drb_id].size(); i ++) {
      standing_queue_sz += drb_pdcp_sn_ts[drb_id][i].size;
    }
    drb_pdcp_sn_ts[drb_id].back().standing_queue_size = standing_queue_sz;
    drb_pdcp_sn_ts[drb_id].back().est_queue_delay = standing_queue_sz / drb_pdcp_sn_ts[drb_id][drb_pdcp_sn_ts[drb_id].size()-1].pred_dequeue_rate;
    printf("standing_queue_sz: %.2f, pred_dequeue_rate: %.2f, est_queue_delay: %.2f\n",
      standing_queue_sz,
      drb_pdcp_sn_ts[drb_id][drb_pdcp_sn_ts[drb_id].size()-1].pred_dequeue_rate,
      drb_pdcp_sn_ts[drb_id][drb_pdcp_sn_ts[drb_id].size()-1].est_queue_delay);
  }

  void make_mark_decision(drb_id_t drb_id) 
  {
    // only update the queue tail's dequeue rate and calculate the queuing delay.
    predict_dequeue_rate(drb_id);
    
    // predict the standing packets' average queuing delay, based on that we make the marking decision.
    predict_queuing_delay(drb_id);

    // change mark decision
    double required_dequeue_rate = drb_pdcp_sn_ts[drb_id].back().standing_queue_size / l4s_tq_thr;
    double predicted_dequeue_rate = drb_pdcp_sn_ts[drb_id].back().pred_dequeue_rate;
    double predicted_error = drb_pdcp_sn_ts[drb_id].back().est_dequeue_rate_error;
    double predicted_qdely = drb_pdcp_sn_ts[drb_id].back().est_queue_delay;

    rx.get()->drb_flow_state[drb_id].predicted_dequeue_rate = predicted_dequeue_rate;
    rx.get()->drb_flow_state[drb_id].required_dequeue_rate = required_dequeue_rate;
    rx.get()->drb_flow_state[drb_id].predicted_error = predicted_error;
    if (predicted_qdely > 0.1) {
      rx.get()->drb_flow_state[drb_id].predicted_qdely = predicted_qdely;
    }
    // printf("cal_cal_dequeue_rate: cal_dequeue_rate: %.2f, est_queue_delay: %.2f\n",
    //   drb_pdcp_sn_ts[drb_id][2].cal_dequeue_rate,
    //   drb_pdcp_sn_ts[drb_id][drb_pdcp_sn_ts[drb_id].size()-1].est_queue_delay);

    logger.log_debug("required_dequeue_rate {}, predicted_dequeue_rate {}, predicted_error {}, est_dequeue_time {}, queue_size {}", 
      required_dequeue_rate, 
      predicted_dequeue_rate, 
      predicted_error,
      predicted_qdely,
      drb_pdcp_sn_ts[drb_id].back().standing_queue_size);

    rx.get()->nof_ue = nof_ue;
    // L4S flow only
    if (rx.get()->drb_flow_state[drb_id].have_l4s) {
    // && !rx.get()->drb_flow_state[drb_id].have_classic){
      if (required_dequeue_rate >  predicted_dequeue_rate + predicted_error) {
        rx.get()->drb_flow_state[drb_id].mark_l4s = RAND_MAX;
      } else if (required_dequeue_rate < predicted_dequeue_rate - predicted_error) {
        rx.get()->drb_flow_state[drb_id].mark_l4s = 0;
      } else {
        rx.get()->drb_flow_state[drb_id].mark_l4s = (uint32_t)(required_dequeue_rate - predicted_dequeue_rate + predicted_error) / 2 / predicted_error * RAND_MAX;
      }
    }

    // Classic flow only
    // double real_classic_tq_thr = classic_tq_thr * sqrt(nof_ue);
    // double required_dequeue_rate_classic = drb_pdcp_sn_ts[drb_id][drb_pdcp_sn_ts[drb_id].size()-1].standing_queue_size / real_classic_tq_thr;
    // double predicted_dequeue_rate_classic = drb_pdcp_sn_ts[drb_id][drb_pdcp_sn_ts[drb_id].size()-1].pred_dequeue_rate;
    // double predicted_error_classic = drb_pdcp_sn_ts[drb_id][drb_pdcp_sn_ts[drb_id].size()-1].est_dequeue_rate_error;
    if (rx.get()->drb_flow_state[drb_id].have_classic) {
      //&& !rx.get()->drb_flow_state[drb_id].have_l4s) {

      uint32_t classic_thres = n_max / nof_ue;
      if (drb_pdcp_sn_ts[drb_id].back().standing_queue_size > classic_thres) {
        // Final design
        // rx.get()->drb_flow_state[drb_id].mark_classic = ((drb_pdcp_sn_ts[drb_id][drb_pdcp_sn_ts[drb_id].size()-1].standing_queue_size - (double) classic_thres) / (double)classic_thres) / 100 *  // * RAND_MAX; 
        //      ((drb_pdcp_sn_ts[drb_id][drb_pdcp_sn_ts[drb_id].size()-1].standing_queue_size - (double)classic_thres) / (double)classic_thres)  / 100 * RAND_MAX;

        // Final design opt-2
        rx.get()->drb_flow_state[drb_id].mark_classic = (1460 * 8 * 1.75 / 2 / predicted_dequeue_rate / predicted_qdely) * 
          (1460 * 8 * 1.75 / 2 / predicted_dequeue_rate / predicted_qdely) * RAND_MAX;
              // ((drb_pdcp_sn_ts[drb_id][drb_pdcp_sn_ts[drb_id].size()-1].standing_queue_size - (double) classic_thres) / (double)classic_thres) / 100 *  // * RAND_MAX; 
              // ((drb_pdcp_sn_ts[drb_id][drb_pdcp_sn_ts[drb_id].size()-1].standing_queue_size - (double)classic_thres) / (double)classic_thres)  / 100 * RAND_MAX;
      } else {
        rx.get()->drb_flow_state[drb_id].mark_classic = 0;
      }

      // /* end_to_end_azure_0523_classic100ms_3 */
      // if (required_dequeue_rate_classic >  predicted_dequeue_rate_classic + predicted_error_classic) {
      //   rx.get()->drb_flow_state[drb_id].mark_classic = RAND_MAX;
      // } else if (required_dequeue_rate_classic < predicted_dequeue_rate_classic - predicted_error_classic) {
      //   rx.get()->drb_flow_state[drb_id].mark_classic = 0;
      // } else {
      //   rx.get()->drb_flow_state[drb_id].mark_classic = (uint32_t)(required_dequeue_rate_classic - predicted_dequeue_rate_classic + predicted_error_classic) / 10 / predicted_error_classic *
      //     (required_dequeue_rate_classic - predicted_dequeue_rate_classic + predicted_error_classic) / 10 / predicted_error_classic * RAND_MAX;
      // }
    }

    // Mixed flows for L4S and Classic
    if (rx.get()->drb_flow_state[drb_id].have_l4s && rx.get()->drb_flow_state[drb_id].have_classic) {
      /// TBD
    }
  }

public:
  struct tcp_packet_info {
    uint32_t seq_num;
    uint32_t end_seq_num;
    uint16_t payload_len;
    int64_t  tx_timestamp_us;
    uint8_t  ecn_mark;
    bool     is_retransmission;
    bool     fake_acked;  // 标记此包是否已被fake ACK过
    
    // 添加完整包的副本
    byte_buffer packet_copy;  // 存储整个 IP 包的副本
    
    // 或者只存储 TCP 载荷
    std::vector<uint8_t> tcp_payload;  // 只存储 TCP 数据部分
  };

  /// Start the periodic timer thread (1ms interval)
  void start_periodic_timer() {
    periodic_timer_running = true;
    periodic_timer_thread = std::thread([this]() {
      logger.log_info("Periodic timer thread started");
      
      while (periodic_timer_running) {
        auto start_time = std::chrono::steady_clock::now();
        
        // Perform periodic tasks
        periodic_task();
        
        // Sleep for 1ms (adjusting for processing time)
        auto end_time = std::chrono::steady_clock::now();
        auto elapsed = std::chrono::duration_cast<std::chrono::microseconds>(end_time - start_time);
        auto sleep_time = std::chrono::microseconds(5000) - elapsed;
        
        if (sleep_time.count() > 0) {
          std::unique_lock<std::mutex> lock(data_mutex);
          timer_cv.wait_for(lock, sleep_time, [this]() { return !periodic_timer_running.load(); });
        }
      }
      
      logger.log_info("Periodic timer thread stopped");
    });
  }
  
  /// Stop the periodic timer thread
  void stop_periodic_timer() {
    if (periodic_timer_running) {
      periodic_timer_running = false;
      timer_cv.notify_all();
      if (periodic_timer_thread.joinable()) {
        periodic_timer_thread.join();
      }
      logger.log_info("Periodic timer stopped, total_checks={}, acks_generated={}", 
                     periodic_stats.total_checks, periodic_stats.acks_generated);
    }
  }
  
  /// Periodic task executed every 1ms
  void periodic_task() {
    std::lock_guard<std::mutex> lock(data_mutex);
    
    auto now = std::chrono::system_clock::now();
    auto duration = now.time_since_epoch();
    auto ts_us = std::chrono::duration_cast<std::chrono::microseconds>(duration).count();
    
    periodic_stats.total_checks++;
    periodic_stats.last_check_timestamp_us = ts_us;
    
    // Task 1: Check RLC queue length and sending rate
    check_rlc_queue_and_rate();
    
    // Task 2: Generate TCP ACKs for received packets
    generate_tcp_acks();
    
    // Log periodically (every 1000 checks = 1 second)
    if (periodic_stats.total_checks % 1000 == 0) {
      logger.log_debug("Periodic stats: checks={}, acks_generated={}", 
                      periodic_stats.total_checks, periodic_stats.acks_generated);
    }
  }
  
  /// Check RLC queue length and sending rate for all DRBs
  void check_rlc_queue_and_rate() {
    // Iterate through all DRBs and check their queue status
    for (auto& [drb_id, pdcp_vec] : drb_pdcp_sn_ts) {
      // Calculate current queue length
      size_t queue_length = 0;
      size_t queue_bytes = 0;
      
      if (next_tx_id.find(drb_id) != next_tx_id.end()) {
        size_t tx_id = next_tx_id[drb_id];
        for (size_t i = tx_id; i < pdcp_vec.size(); i++) {
          queue_length++;
          queue_bytes += pdcp_vec[i].size;
        }
      }
      
      // Get current sending rate (from last prediction)
      double sending_rate_bps = 0.0;
      if (!pdcp_vec.empty() && pdcp_vec.back().pred_dequeue_rate > 0) {
        sending_rate_bps = pdcp_vec.back().pred_dequeue_rate * 8; // Convert to bits per second
      }
      
      // Log queue status if there's something in the queue
      if (queue_length > 0) {
        logger.log_debug("DRB {} queue: packets={}, bytes={}, rate={} bps", 
                        drb_id, queue_length, queue_bytes, sending_rate_bps);
      }
      
      // TODO: Add interface to query actual RLC layer queue status
      // This requires adding a callback or interface to RLC layer
      // Example:
      // if (rlc_interface) {
      //   auto rlc_status = rlc_interface->get_queue_status(drb_id);
      //   logger.log_debug("RLC queue: bytes={}, packets={}", 
      //                   rlc_status.queue_bytes, rlc_status.queue_packets);
      // }
    }
  }
  
  /// Generate TCP ACKs for received packets that need acknowledgment
  void generate_tcp_acks() {
    if (!rx) {
      return;  // RX not initialized yet
    }
    
    // Iterate through all TCP flows and check if we need to generate ACKs
    for (auto& [five_tuple, flow_track] : rx->tcp_flow_tracking) {
      // Check if there are new packets
      if (
          flow_track.last_fake_ack < flow_track.next_expected_seq) {
        
        // Find the QFI for this flow
        auto drb_it = rx->five_tuple_to_drb.find(five_tuple);
        if (drb_it == rx->five_tuple_to_drb.end()) {
          logger.log_debug("Flow not found in five_tuple_to_drb map");
          continue;
        }
        
        drb_id_t drb_id = drb_it->second.drb_id;
        
        // Find QFI from drb_id (reverse lookup)
        qos_flow_id_t target_qfi = qos_flow_id_t::min;
        bool found_qfi = false;
        for (auto& [qfi, mapped_drb] : qfi_to_drb) {
          if (mapped_drb == drb_id) {
            target_qfi = qfi;
            found_qfi = true;
            break;
          }
        }
        
        if (!found_qfi) {
          logger.log_debug("QFI not found for drb_id={}", drb_id);
          continue;
        }
        
        // Generate our sequence number using the first un-fake-acked packet's sequence number in the queue
        // In real implementation, should track our own seq numbers per flow
        uint32_t our_seq_num = 0;  // Default value
        bool found_unfaked_packet = false;
        for (auto& pkt : flow_track.in_flight_packets) {
          if (!pkt.fake_acked) {
            our_seq_num = pkt.seq_num;
            pkt.fake_acked = true;  // Mark this packet as fake ACKed
            found_unfaked_packet = true;
            flow_track.last_fake_ack = our_seq_num;
            break;
          }
        }
        
        // If no unfaked packets found, skip generating ACK
        if (!found_unfaked_packet) {
          continue;
        }
        
        // Construct TCP ACK packet
        byte_buffer ack_packet = construct_tcp_ack(
            five_tuple,
            flow_track.last_ack_received,  // ACK number
            our_seq_num,                    // Our SEQ number
            flow_track.last_ack_timestamp_us
        );
        
        if (ack_packet.length() > 0) {
          logger.log_info("Generating TCP ACK: flow={}, ack_seq={}, qfi={}, packets_acked={}", 
                          five_tuple, 
                          flow_track.last_ack_received,
                          target_qfi,
                          flow_track.total_packets_acked);
          
          // Send the ACK packet via rx_sdu_notifier
          rx->sdu_notifier.on_new_sdu(std::move(ack_packet), target_qfi);
          periodic_stats.acks_generated++;
          
          // Reset ACK counter to avoid sending duplicate ACKs
          // (In real implementation, should track what's been ACKed)
          flow_track.total_packets_acked = 0;
        } else {
          logger.log_error("Failed to construct TCP ACK packet for flow={}", five_tuple);
        }
      }
    }
  }
  
  /// Helper function to construct TCP ACK packet
  byte_buffer construct_tcp_ack(const ip::five_tuple& flow, 
                                 uint32_t ack_num, 
                                 uint32_t seq_num,
                                 int64_t timestamp_us) {
    byte_buffer ack_pkt;
    
    // Allocate buffer for IP + TCP headers (40 bytes minimum for ACK without options)
    const size_t ip_header_size = sizeof(iphdr);
    const size_t tcp_header_size = sizeof(tcphdr);
    const size_t total_size = ip_header_size + tcp_header_size;
    
    // Append space for headers
    if (!ack_pkt.append(std::vector<uint8_t>(total_size, 0))) {
      logger.log_error("Failed to allocate buffer for TCP ACK packet");
      return ack_pkt;
    }
    
    // Get pointer to buffer data
    auto seg_it = ack_pkt.segments().begin();
    if (seg_it == ack_pkt.segments().end()) {
      logger.log_error("Invalid buffer segments for TCP ACK");
      return byte_buffer();
    }
    uint8_t* pkt_data = const_cast<uint8_t*>((*seg_it).data());
    
    // Construct IP header (reverse direction: dst -> src)
    iphdr* ip_hdr = reinterpret_cast<iphdr*>(pkt_data);
    ip_hdr->version = 4;
    ip_hdr->ihl = 5;  // 5 * 4 = 20 bytes
    ip_hdr->tos = ip::INET_ECN_ECT_0;  // ECN capable
    ip_hdr->tot_len = total_size;
    ip_hdr->id = 0;  // Can be 0 for ACK packets
    ip_hdr->frag_off = 0;
    ip_hdr->ttl = 64;
    ip_hdr->protocol = 6;  // TCP
    ip_hdr->check = 0;  // Will calculate later
    // Reverse src/dst (ACK goes back)
    ip_hdr->saddr = flow.dst_addr;
    ip_hdr->daddr = flow.src_addr;
    
    // Construct TCP header (reverse direction: dst -> src)
    tcphdr* tcp_hdr = reinterpret_cast<tcphdr*>(pkt_data + ip_header_size);
    tcp_hdr->source = flow.dst_port;
    tcp_hdr->dest = flow.src_port;
    tcp_hdr->seq = seq_num;  // Our sequence number
    tcp_hdr->ack_seq = ack_num;  // Acknowledging their data
    tcp_hdr->doff = 5;  // 5 * 4 = 20 bytes (no options)
    tcp_hdr->res1 = 0;
    tcp_hdr->cwr = 0;
    tcp_hdr->ece = 0;
    tcp_hdr->urg = 0;
    tcp_hdr->ack = 1;  // ACK flag set
    tcp_hdr->psh = 0;
    tcp_hdr->rst = 0;
    tcp_hdr->syn = 0;
    tcp_hdr->fin = 0;
    tcp_hdr->window = 65535;  // Advertise large window
    tcp_hdr->check = 0;  // Will calculate later
    tcp_hdr->urg_ptr = 0;
    
    // Calculate checksums (need to swap before checksum calculation)
    ip::swap_iphdr(ip_hdr);
    ip::swap_tcphdr(tcp_hdr);
    
    // Calculate IP checksum
    ip_hdr->check = ip::compute_ip_checksum(ip_hdr);
    
    // Calculate TCP checksum
    tcp_hdr->check = ip::compute_tcp_checksum(ip_hdr, tcp_hdr, pkt_data);
    
    logger.log_debug("Constructed TCP ACK: src={}:{}, dst={}:{}, seq={}, ack={}", 
                    flow.dst_addr, flow.dst_port, flow.src_addr, flow.src_port,
                    seq_num, ack_num);
    
    return ack_pkt;
  }
};
